[["index.html", "Introduction to Data Science in R Chapter 1 Preface", " Introduction to Data Science in R Joseph M. Westenberg 2021-07-19 Chapter 1 Preface These notes are a first draft and quite prelimenary. If you find errors, please let me know by emailing jwesten@iu.edu An example of my code for this course is avaialabe on my github repository at https://github.com/r-introtodatascience/sample_repo. The goal of these short courses are to give graduate students a very prelimenary introduction into coding in R. While having an instructor teach you these things may be helpful, in my belief, coding, like math, you have to learn by doing. So I highly encourage following these notes, but then play around with the code. Change things and see how it changes the output. I think this is the best way to learn. Over the next 5 sessions we will be looking into county level unemployment data. The goal will be to provide examples and descriptions of the commands and techniques I have discovered to be most useful while working with data. Specifically the goal is by the end of the sessions it is my hope that you will know how to: download data straight into R read in data (Rda, csv, xlsx, dta) merge datasets subset datasets create summary statistics and plots run basic regressions all while paying particular attention to file pathing and to teach you how to export graphics and tables from R into a form that can be directly read into LaTex (so that it can be easily read into paper/presentation documents). Below is an example of an interactive plot you can make with a package called plotly. Try interacting with it! In the plot you can select/deselect states you want to view. There is also a link to a short post I made about using GIS with R. I show these to illustrate a bit of what is possible to do in R after we familiarize ourselves with the basic functions. "],["intro.html", "Chapter 2 Introduction 2.1 Very Basics 2.2 Writing Loops 2.3 String Manipulation 2.4 Project Organization 2.5 Downloading within R 2.6 Reading in Data 2.7 Download Loop", " Chapter 2 Introduction The goal of this section is to provide a very quick introduction or refresher to R. If this is your first time coding, this brief introduction will most likely be quite insufficient. I encourage you to seek other resources to learn the basics before proceeding. In particular we will cover: Writing Loops String Manipulation Project Organization Downloading/Reading Data Downloading Loop In this lesson we will be downloading and cleaning data from the BLS on county level employment statistics from 1990-2019. 2.1 Very Basics I assume you have R installed and running. There are plenty of guides online on how to do this. Lets first define some arrays within R. These can be numeric based, in this case integer. numvec1&lt;-c(5,6,7) # 1 numvec2&lt;-c(7,8,9) # 2 Lets dig into what is going on a bit more. We are telling R to define a vector, this is the c( ) part, with elements 1,2,3 and give that vector a name A. The backwards arrow tells R what is the name and what is the element we are defining. We can make character based vectors as well. charvec1&lt;-c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) # 1 charvec2&lt;-c(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;) # 2 We can then combine these vectors into a dataframe (this is relevant for when we start thinking about reading in/manipulating actual data). Since all of our vectors are length three, we can easily create a dataframe (think a matrix) where our column names will be the name of the vectors, and the rows will be the elements of the vectors. first_dataframe&lt;-data.frame(numvec1,numvec2,charvec1,charvec2) # 1 print(first_dataframe) # 2 ## numvec1 numvec2 charvec1 charvec2 ## 1 5 7 a d ## 2 6 8 b e ## 3 7 9 c f Now first_dataframe is going to be of similar format as we will typically have when we read in data from excel files into R. We can access certain rows and columns within the dataframe by putting square brackets after the name of the dataframe. For example if we wanted to print the element in the first row and first column, we could define the variable x as this and then print x. (Keep in mind the ordering is rows, columns) x&lt;-first_dataframe[1,1] # 1 print(x) # 2 ## [1] 5 What if we wanted to print all elements in the first row, we just leave the column (after the comma) blank: x&lt;-first_dataframe[1,] # 1 print(x) # 2 ## numvec1 numvec2 charvec1 charvec2 ## 1 5 7 a d How about 1st &amp; 3rd row? x&lt;-first_dataframe[c(1,3),] # 1 print(x) # 2 ## numvec1 numvec2 charvec1 charvec2 ## 1 5 7 a d ## 3 7 9 c f Another way to do this is to define another variable, say y as a vector with elements 1 and 3. Notice how the below output is the same as the above. y&lt;-c(1,3) # 1 x&lt;-first_dataframe[y,] # 2 print(x) # 3 ## numvec1 numvec2 charvec1 charvec2 ## 1 5 7 a d ## 3 7 9 c f We can do the same thing for the columns (we need to remember the order for the square brackets are rows, columns). Note: if we put a negative sign in front of these commands in the brackets, instead keeping certain rows or columns, it means remove! That is if in the below command we have -3, it would be saying REMOVE column 3! x&lt;-first_dataframe[,3] # 1 y&lt;-first_dataframe[,-3] # 2 print(x) # 3 ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; print(y) # 1 ## numvec1 numvec2 charvec2 ## 1 5 7 d ## 2 6 8 e ## 3 7 9 f Our third column is named C, we can also pull this column by referencing its name after a dollar sign, such as: x&lt;-first_dataframe$charvec1 # 1 print(x) # 2 ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; This may not sound useful now, but think if we have many columns of variables, say wage, hoursworked, fulltime, and hundreds of more. We dont want to have to find what column number hoursworked is, we can just reference this column name. 2.1.0.1 And and Or Operators A couple common operators that we may want to use are and and or statements. Within R: &amp; : Our and operator | : Our or operator For example: first_dataframe[first_dataframe$numvec1&gt;=6 &amp; first_dataframe$numvec2&lt;9,] # 1 ## numvec1 numvec2 charvec1 charvec2 ## 2 6 8 b e first_dataframe[first_dataframe$numvec1&gt;=6 | first_dataframe$numvec2&lt;9,] # 1 ## numvec1 numvec2 charvec1 charvec2 ## 1 5 7 a d ## 2 6 8 b e ## 3 7 9 c f 2.2 Writing Loops letters&lt;-c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # 1 letters_l&lt;-length(letters) # 2 for (i in 1:letters_l){ # 3 print(letters[i]) # 4 } # 5 ## [1] &quot;a&quot; ## [1] &quot;b&quot; ## [1] &quot;c&quot; ## [1] &quot;d&quot; ## [1] &quot;e&quot; Going line by line: Line 1: Define a vector of letters. Line 2: Report the number of elements in our letter vector and save as letters_l Line 3: Defining for loop. Our index will be i, and it will run from 0 to however long letters vector is (try adding some more letters!) Note our for loop action is defined within the curly braces. Line 4: For every i defined in Line 3 we want to print the corresponding element in the vector letters. Okay, great. How does this help us with reading in the data? Well get to that in the next section. 2.3 String Manipulation 2.3.1 String Concatenation a&lt;-&quot;This is a the start&quot; # 1 b&lt;-&quot;of a sentence&quot; # 2 print(paste0(a,b)) # 3 ## [1] &quot;This is a the startof a sentence&quot; print(paste(a,b)) # 1 ## [1] &quot;This is a the start of a sentence&quot; Notice we define a and b as strings. What paste and paste0 do are combine these strings into one string. We can see that paste places a space between the two strings while paste0 does not. paste0 comes in quite handy for working with file pathing as we will see. Yes its that easy! We will not be using this for this project but it may be useful to know we can concatenate two vectors of strings as well. a&lt;-c(&quot;This is a the start&quot;, &quot;Now we have&quot;, &quot;This really is&quot;, &quot;Economics is&quot;) # 1 b&lt;-c(&quot;of a sentence.&quot;, &quot;another sentence.&quot;, &quot;quite handy.&quot;, &quot;awesome!&quot;) # 2 print(paste(a,b)) # 3 ## [1] &quot;This is a the start of a sentence.&quot; &quot;Now we have another sentence.&quot; ## [3] &quot;This really is quite handy.&quot; &quot;Economics is awesome!&quot; 2.3.2 String Padding Consider we have a vector of numbers which currently runs 1-19. Now what if we need all the single character digits to have a leading zero. That is instead of 1 we need 01. We could use paste0 as above and combine a 0 with our vector. numbvec&lt;-as.character(1:19) # 1 print(paste0(&quot;0&quot;, numbvec)) # 2 ## [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;010&quot; &quot;011&quot; &quot;012&quot; ## [13] &quot;013&quot; &quot;014&quot; &quot;015&quot; &quot;016&quot; &quot;017&quot; &quot;018&quot; &quot;019&quot; But we dont want a leading 0 in front of the double character digits (ie We DONT want 090). We could break our vector into single character digits and two character digits, manipulate the single character digits, then combine it back in with the double character digits. But there is an easier way: str_pad ! require(stringr) # 1 numbvec&lt;-as.character(1:19) # 2 print(str_pad(numbvec, 2, &quot;left&quot;, &quot;0&quot;) ) # 3 ## [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; Now this looks like what we want! But what is str_pad doing? With str_pad we are telling stringr we want all elements of numbvec to be of length 2. So stringr checks to see if the elements are less than 2 characters, if an element is it adds 0s to the left side until it reaches length 2. If it is already length 2, it will leave it alone. There is many other handy commands to deal with strings in R. These are just a couple of commands we will be using. I will be writing a post with some other handy functions in the coming weeks and will link it here. 2.4 Project Organization Before we get started, lets set up a folder for our project and create subfolders to keep things organized. For this project I recommend the following subfolders, which are a good minimum for organizing any project: raw_data: This is where we will put our `preprocessed data we will be getting from MIT Election Lab. scripts: Where we will save all of our R-scripts in this folder processed_data: If we want to save some intermediate data steps between raw data and our output. tables: Any tex tables we generate we will save to this folder figures: Any figures we generate we will save to this folder tex: Where we can have our paper and/or presentations You can make more folders if you feel it keeps you organized. The main point I want to make here is it is well worth your time to think about how you want to organize your project. Oftentimes if we jump right in without a plan, things become a jumbled mess. (If you want to go further with your organization strategies, I recommend looking into waf, specifically check out Templates for Reproducible Research Projects in Economics.) My project folder now looks like this: 2.4.1 First R script! Now that we have our folders created, lets now create paths to those folders with R. Lets start by creating a variable workingdir which we can define as the path to our main folder. workingdir&lt;-&quot;PATH_TO_YOUR_WORKING_DIR&quot; # 1 Now we are going to create a script, lets save it as workingdir.R and place it in our main parent folder. We are then going to be using the path we defined above in this script to create paths to our other folders. folder_figures&lt;-paste0(workingdir, &quot;figures&quot;) # 1 folder_processed_data&lt;-paste0(workingdir, &quot;processed_data&quot;) # 2 folder_raw_data&lt;-paste0(workingdir, &quot;raw_data&quot;) # 3 folder_scripts&lt;-paste0(workingdir, &quot;scripts&quot;) # 4 folder_tables&lt;-paste0(workingdir, &quot;tables&quot;) # 5 folder_tex&lt;-paste0(workingdir, &quot;tex&quot;) # 6 Now at the top of all other scripts we can have: workingdir&lt;-&quot;PATH_TO_YOUR_WORKING_DIR&quot; # 1 source(paste0(workingdir, workingdir.r)) # 2 Now all this says is we have a variable named workingdir that points to our main parent folder. We then have a script in which we define paths to all other folders. With this at the front of all of our scripts, we can easily reference them to create easier pathing for ourselves. 2.5 Downloading within R We can use R to with a direct link to download. The first argument the download.file() command takes that we will use is the url of the xslx document and the second argument is the destination it will be saved. The last argument is basically telling R that the excel docs are not plain text. (Dont forget to have workingdir defined as we did in Project Organization download.file(&quot;https://www.bls.gov/lau/laucnty90.xlsx&quot;, # 1 paste0(folder_raw_data, &quot;bls_unemp_90.xlsx&quot;), mode=&quot;wb&quot;) # 2 2.6 Reading in Data Before we read in this data to R, lets see what we are dealing with. Opening up the file in excel we can see there will be issues reading the file in. We can notice that the first row has the file title spread across columns A:J. Variable names are spread across anywhere 1-3 rows. And lastly we have an empty column in F. The bright side is if we observe the other years files, they all have this exact same structure. Hence we will able to use a loop eventually to clean them all instead of one at a time. The main package we will be using is readxl, which is quite self explanatory. It is a package meant to help to read in excel docs. Lets try to open the file for 1990 we downloaded in R. We can do this through R Studios functionality. Within the Environment area of R Studio, click Import Dataset, then From Excel There is definitely multiple ways to do this, as we can see from the options available. I first deselect First Row as Names (This option is very nice if your data is already in a precleaned form and your first row simply has your variable names.) I then begin to skip rows, 5 rows of skipping leads to the first row being the first row of data. Next we can handle column F that we noticed was blank. This is column 6 and stays consistent across all years (you can check this). Remembering our lessons from the Very Basics section we can subset this dataframe by removing column 6. Hence we have something that will look like the following for our command for our script. require(readxl) # 1 workingdir&lt;-&quot;C:/Users/weste/Documents/GitHub/r-introtodatascience/sample_repo/&quot; # 2 bls_unemp_90 &lt;- read_excel(paste0(folder_raw_data, &quot;/bls_unemp_90.xlsx&quot;), # 3 col_names=FALSE, skip=5) # 4 bls_unemp_90&lt;-bls_unemp_90[,-6] # 5 colnames(bls_unemp_90)&lt;-c(&quot;LAUS_code&quot;, &quot;State_fips&quot;, &quot;County_fips&quot;, &quot;County_name&quot;, # 6 &quot;Year&quot;, &quot;Labor_force&quot;, &quot;Employed&quot;, &quot;Unemployed&quot;, &quot;Unemp_rate&quot;) # 7 Where the last line above we are giving our columns names based on the names we saw in the excel document. We can then observe our data frame to ssee if its fully cleaned. At first it seems so (I actually initially thought so). However when reading in the data, we grabbed 2 extra rows at the end of the file. Hence we have 2 rows at the end of our data frame that are NAs. Lets drop these two rows, to do this we can use a command is.na and prior to it include an ! saying not is.na. bls_unemp_90&lt;-bls_unemp_90[!is.na(bls_unemp_90$State_fips),] # 1 Here is a good place to pause if you want a challenge. You should have all the tools needed to write a loop to download all files from 1990-2019. 2.7 Download Loop In the next sections we will be using county level employment/labor force data from BLS to learn more about working with actual data. We will be using the Labor force data by county, yearly annual averages. There is data from 1990-2019 (as of writing these notes). To start we are going to download this data and then read it into R. We can use a loop to download/clean our data such as this: require(stringr) # 1 require(readxl) # 2 unemp_data&lt;-data.frame() # 3 years&lt;-c(90:99, 0:19) # 4 years&lt;-str_pad(as.character(years), 2, &quot;left&quot;, &quot;0&quot;) # 5 years_l&lt;-length(years) # 6 for (i in 1:years_l){ # 7 url&lt;-paste0(&quot;https://www.bls.gov/lau/laucnty&quot;, years[i], &quot;.xlsx&quot;) # 8 destination&lt;-paste0(folder_raw_data, &quot;/bls_unemp_&quot;, years[i], &quot;.xlsx&quot;) # 9 download.file(url, destination, mode=&quot;wb&quot;) # 10 temp_df &lt;- read_excel(paste0(folder_raw_data, &quot;/bls_unemp_&quot;, years[i], # 11 &quot;.xlsx&quot;),col_names=FALSE, skip=5) # 12 temp_df&lt;-temp_df[,-6] # 13 colnames(temp_df)&lt;-c(&quot;LAUS_code&quot;, &quot;State_fips&quot;, &quot;County_fips&quot;, &quot;County_name&quot;, # 14 &quot;Year&quot;, &quot;Labor_force&quot;, &quot;Employed&quot;, &quot;Unemployed&quot;, &quot;Unemp_rate&quot;) # 15 temp_df&lt;-temp_df[!is.na(temp_df$State_fips),] # 16 unemp_data&lt;-rbind(county_data, temp_df) # 17 } # 18 filename&lt;-paste0(folder_processed_data, &quot;/unemp_data.rda&quot;) # 19 save(unemp_data, file=filename) # 20 To work through this first lets think what we are trying to achieve. The links for the downloads are all in the form of https://www.bls.gov/lau/laucntyZZ.xlsx, where ZZ is two digits representing the year. These ZZ values run from 90to 99 for years 1990-1999 and 00 to 19 for years 2000-2019. Lets work through the above code line by line: Lines #1 &amp; #2: load required packages. Line #3: Declare unemp_data will be a data.frame. Right now it is empty, but we will add to it. Line #4: Define a vector with elements 90-99 and 0-19. (Which will correspond to the years that we will pull) Line #5 : We have a vector for years, however if we notice in the url names we need this vector to include a leading 0 in front of the single character digits (ie 01 instead of 1). But we dont want a leading 0 in front of the double character digits (ie We DONT want 090). Go back to the String Manipulation section if you need to refresh on this. Line #6 : Calculate the length of years and save as years_l Line #7 : See String Manipulation if defining the for loop does not make sense. Line #8 : We are creating the character string for the url for the download link. Since they all take the form of https://www.bls.gov/lau/laucntyZZ.xlsx, we can use one element of our years vector at a time. (See String Manipulation for explanation on paste0) Line #9 : This is of similar spirit to line #4, but this is defining the path/filename of the excel file we will save. Line #10 : This line is just telling R to download the file at that url, save it to the defined location/name, and to read it as a non-raw text form. (see Downloading/Reading Data if unclear.) Lines #11-#16: See Downloading/Reading Data for a direct explanation. Line #17: rbind appends data. Hence since all of our data has the same format and has a variable indicating the year, we can simply append. Line #19: Create the filepath (to our processed_data folder) where we will save the file, what the file name and type is. Line #20: save the combined data to the location/name we defined above. We now have our data cleaned and saved for our next lesson when we will start to work with it more! "],["tidyverse.html", "Chapter 3 Tidyverse 3.1 Filtering with Pipes 3.2 Plotting Data 3.3 Manipulating within Groups 3.4 Defining New Variables 3.5 Dropping Variables 3.6 Merging Datasets", " Chapter 3 Tidyverse The goal of this Tidyverse section is to learn Subsetting Datasets Merging Datasets Summarizing Plotting In this section we will be continuing to be working with the BLS data we downloaded in the Introduction. Tidyverse provides us with some useful tools for data manipulation and cleaning. I include examples and descriptions of the commands I have most frequently used. 3.1 Filtering with Pipes indiana&lt;- # 1 unemp_data %&gt;% filter(State_fips==&quot;18&quot;) # 2 What is going on in this line of code? Lets start with the second line. The operator %&gt;% is called a pipe. This comes from a package called magittr which is included within tidyverse. Pipes are very handy for neatly writing longer sequences of code. Here we begin with a pretty simple example. We can think of pipes as saying then. That is it takes the argument before it and uses it as input in the following command. Here we are using a filter command, so we are saying take our data.frame county_data, then filter or give us the values where State_fips is equal to 18 (this is fips code for Indiana. Go Hoosiers!). So in summary we are just filtering out indiana data and saving it as a new data frame (this is line 1). Note: this operation does NOT impact county_data, the only way we over write a data frame is to explicitly tell it to. For example the following WOULD overwrite county_data. unemp_data&lt;-unemp_data %&gt;% filter(State_fips==&quot;18&quot;) # 1 3.2 Plotting Data In this lesson we will be plotting the data gathered in previous lessons. ggplot2 is a very popular package for plotting and is now included as part of the tidyverse package. It allows us to make many different kinds of plots and customize them. As it is so popular there are many guides for it. Tidyverses website summarizes some very useful resources. As ggplot2 has so many options, we will just be barely skimming the surface in this introduction. Lets start with a minimal command. First lets load in ggplot2 filter our data down into just Indiana. library(ggplot2) # 1 # 2 indiana &lt;- unemp_data %&gt;% filter(State_fips==&quot;18&quot;) # 3 ggplot(data=indiana, aes(x=Year, y=Unemp_rate, # 1 colour=County_fips, group=County_fips))+ # 2 geom_line() # 3 Line #1: Tells ggplot we will be using the data.frame indiana. aes is short for aesthetics. This is where we tell ggplot how to map our data into the plots. Here I want ggplot to plot Year on the x-axis, Unemp_rate on the y-axis. Line #2: A continuation of the aesthetic mapping, his line is saying plot separately for each County_fips separately, giving them a different color. Line #3: Tells ggplot to plot the data as lines. First thing after this plot, the legend is excessive. Lets just suppress it for now. To do this we can define the legend.position in theme as none. The theme command allows for a lot of customization with the plot. If you want to see all options click here. Lets also change the thickness of the lines, we can do this with the alpha argument. ggplot(data=indiana, aes(x=Year, y=Unemp_rate, # 1 colour=County_fips, group=County_fips))+ # 2 geom_line(size=0.3)+ # 3 theme(legend.position=&quot;none&quot;) # 4 This already is looking a lot better. But can we do something about the labels on the x axis, they are overlaying on each other. We could make the text smaller but then it would be difficult to see, so lets just rotate them. Also lets make the y-axis name a bit more meaningful. ggplot(data=indiana, aes(x=Year, y=Unemp_rate, # 1 colour=County_fips, group=County_fips))+ # 2 geom_line(size=0.3)+ # 3 theme(legend.position=&quot;none&quot;, # 4 axis.text.x=element_text(angle=45)) + # 5 ylab(&#39;Indiana Unemployment Rate by County&#39;) # 6 Now were getting somewhere! But what if we want to emphasize the county Bloomington is in, Monroe County? monroecty&lt;-indiana %&gt;% filter(County_fips==&quot;105&quot;) # 1 # 2 ggplot(data=indiana, aes(x=Year, y=Unemp_rate, # 3 colour=County_fips, group=County_fips))+ # 4 geom_line(size=0.3)+ # 5 geom_line(data=monroecty, aes(x=Year, y=Unemp_rate, # 6 colour=County_fips, group=1), # 7 size=0.6, colour=&#39;black&#39;) + # 8 theme(legend.position=&quot;none&quot;, # 9 axis.text.x=element_text(angle=45)) + # 10 ylab(&#39;Indiana Unemployment Rate by County&#39;) # 11 We can make the black line for Monroe County pop a bit more too. Lets change the opacity of the other counties lines. To do this we can set a value, alpha. This takes values 0 to 1 where smaller values are more transparent. ggplot(data=indiana, aes(x=Year, y=Unemp_rate, # 1 colour=County_fips, group=County_fips))+ # 2 geom_line(size=0.3, alpha=0.5)+ # 3 geom_line(data=monroecty, aes(x=Year, y=Unemp_rate, # 4 colour=County_fips, group=1), # 5 size=0.6, colour=&#39;black&#39;) + # 6 theme(legend.position=&quot;none&quot;, # 7 axis.text.x=element_text(angle=45)) + # 8 ylab(&#39;Indiana Unemployment Rate by County&#39;) # 9 3.2.1 Scatter plot (geom_point) What if we want points to insteaed of lines for each year? ggplot(data=indiana, aes(x=Year, y=Unemp_rate, # 1 colour=County_fips, group=County_fips))+ # 2 geom_point() # 3 3.3 Manipulating within Groups Our next chunk of code gets a bit more complex, lets take it peice by peice though. indiana %&gt;% group_by(Year) %&gt;% # 1 summarise(Average=mean(Labor_force)) # 2 Lets again take it from the end and work our ways backwards through it. The 3rd line above is calculating a new variable within a group that is called Average. The group (line 2) that we are calculating within is each Year for our Indiana data.frame. That is this snippet of code will give us one value back for each year, the average Labor Force level across Indianas counties. Year Average 1990 30593.65 1991 30357.00 1992 31058.60 1993 32046.84 1994 33308.01 1995 34078.00 1996 33800.80 1997 33919.72 1998 33947.47 1999 33965.91 2000 33982.50 2001 34140.32 2002 34469.34 2003 34597.76 2004 34432.72 2005 34841.75 2006 35164.35 2007 34866.21 2008 35131.49 2009 34717.45 2010 34512.97 2011 34586.91 2012 34454.88 2013 34656.59 2014 35052.00 2015 35508.30 2016 36178.11 2017 36243.96 2018 36756.59 2019 36819.25 This group_by command paired with summarise is quite useful. For example going back to our original data.frame, county_data. Suppose we wanted to do what we just did but for all states. That is calculate within each state and year combination. With the group_by command this is quite simple! (I print the first 100 lines of output below) unemp_data %&gt;% # 1 group_by(Year, State_fips) %&gt;% # 2 summarise(Average=mean(Labor_force)) # 3 ## `summarise()` has grouped output by &#39;Year&#39;. You can override using the `.groups` argument. Year State_fips Average 1990 01 28468.343 1990 02 10323.923 1990 04 119079.067 1990 05 15091.147 1990 06 261007.414 1990 08 27557.641 1990 09 227471.000 1990 10 120074.667 1990 11 328924.000 1990 12 96480.269 1990 13 20653.579 1990 15 137730.500 1990 16 11193.045 1990 17 58304.049 1990 18 30593.652 1990 19 14664.212 1990 20 12119.305 1990 21 14619.667 1990 22 29216.547 1990 23 39563.625 1990 24 107919.000 1990 25 229027.357 1990 26 55509.072 1990 27 27573.690 1990 28 14438.854 1990 29 22737.852 1990 30 7198.929 1990 31 8871.215 1990 32 39407.176 1990 33 62009.100 1990 34 193117.000 1990 35 21630.061 1990 36 142149.565 1990 37 34765.130 1990 38 5985.226 1990 39 61586.670 1990 40 19786.338 1990 41 41408.833 1990 42 86992.045 1990 44 105072.600 1990 45 37903.326 1990 46 5260.303 1990 47 25207.768 1990 48 33932.858 1990 49 28352.138 1990 50 21768.214 1990 51 24452.541 1990 53 64751.974 1990 54 13857.600 1990 55 35970.542 1990 56 10259.348 1990 72 14517.090 1991 01 28665.134 1991 02 10626.000 1991 04 120334.200 1991 05 15060.213 1991 06 260801.552 1991 08 28035.766 1991 09 230137.750 1991 10 120571.333 1991 11 321433.000 1991 12 97499.104 1991 13 20619.044 1991 15 142897.250 1991 16 11547.432 1991 17 58277.431 1991 18 30357.000 1991 19 14896.960 1991 20 12190.105 1991 21 14689.508 1991 22 29804.094 1991 23 40318.625 1991 24 109023.333 1991 25 228211.357 1991 26 55369.482 1991 27 27903.586 1991 28 14439.512 1991 29 23000.835 1991 30 7247.179 1991 31 8992.527 1991 32 41338.118 1991 33 61429.900 1991 34 193610.571 1991 35 21974.485 1991 36 140985.613 1991 37 35380.200 1991 38 5947.509 1991 39 61726.409 1991 40 19575.922 1991 41 42135.556 1991 42 87390.030 1991 44 103603.400 1991 45 38234.348 1991 46 5303.182 1991 47 25407.611 1991 48 34512.854 1991 49 29390.414 1991 50 21683.857 1991 51 25107.677 1991 53 65281.667 Average is just one statistic we may want to calculate, how about percentiles, minimum, maximum, standard deviation. our summarise command will accept more than one argument, we just have to separate by a comma. Lets also save this as a new data.frame for use in future lessons. indiana_laborforce&lt;- # 1 indiana %&gt;% group_by(Year) %&gt;% # 2 summarise(Min=min(Labor_force), # 3 p10th=quantile(Labor_force, c(0.1)), # 4 p25th=quantile(Labor_force, c(0.25)), # 5 p50th=quantile(Labor_force, c(0.5)), # 6 p75th=quantile(Labor_force, c(0.75)), # 7 p90th=quantile(Labor_force, c(0.9)), # 8 Max=max(Labor_force), # 9 Average=mean(Labor_force), # 10 StDev=sd(Labor_force)) # 11 Year Min p10th p25th p50th p75th p90th Max Average StDev 1990 2629 6639.3 9617.50 14858.5 30089.50 60784.5 424053 30593.65 52989.19 1991 2541 6380.5 9480.00 14713.0 30131.00 62309.9 421789 30357.00 52670.08 1992 2550 6284.2 9615.50 15169.5 31117.00 63623.2 429239 31058.60 53551.02 1993 2607 6200.6 10037.75 15652.5 32034.75 64549.9 438377 32046.84 54683.97 1994 2681 6453.1 10480.50 16472.0 33808.25 66506.3 453457 33308.01 56427.35 1995 2714 6543.2 10994.75 16648.5 34588.00 67364.5 459603 34078.00 57219.27 1996 2724 6522.6 10827.50 16551.0 34597.25 66487.5 453728 33800.80 56670.37 1997 2698 6585.3 10966.00 16671.5 34786.50 66195.3 453928 33919.72 56870.28 1998 2701 6629.7 11008.75 16660.0 34150.50 65887.6 452412 33947.47 56827.82 1999 2691 6496.7 10986.75 16958.5 33968.50 65196.0 448732 33965.91 56434.89 2000 2985 7053.5 10816.50 16984.5 34996.50 63718.6 455135 33982.50 56818.02 2001 2982 7085.7 10709.00 17127.5 34674.75 64832.0 459505 34140.32 57318.59 2002 3051 7040.7 10802.75 17244.5 34974.75 66572.3 462330 34469.34 57807.67 2003 3092 6941.0 10709.50 17199.0 34514.75 67751.5 464331 34597.76 58056.22 2004 3109 6815.2 10547.00 17083.5 33431.50 68476.2 459140 34432.72 57583.09 2005 3152 7075.8 10702.75 17357.5 33606.00 69985.6 459492 34841.75 57925.42 2006 3107 6883.1 10620.50 17326.5 34554.50 72792.0 461468 35164.35 58484.53 2007 3026 6816.7 10447.50 16881.0 34304.50 73128.1 460046 34866.21 58342.88 2008 3044 6816.2 10488.00 16673.5 34065.50 75072.0 463200 35131.49 58819.13 2009 3088 6758.4 10434.25 16452.0 33700.25 74434.1 457524 34717.45 58083.80 2010 3263 6440.5 10308.00 16622.0 33921.75 74406.0 452836 34512.97 57806.16 2011 3210 6459.9 10184.25 16630.5 33804.00 75620.5 456011 34586.91 58199.53 2012 3181 6336.2 10058.50 16453.0 32924.00 76458.0 458377 34454.88 58388.80 2013 3165 6325.9 9900.50 16289.5 33053.00 78149.5 463346 34656.59 58983.65 2014 3216 6401.2 9851.50 16524.0 33215.50 80113.3 467169 35052.00 59579.47 2015 3178 6341.8 9924.75 16908.0 33960.00 82327.7 472330 35508.30 60275.44 2016 3226 6444.8 9930.50 17093.5 35149.00 85223.0 481434 36178.11 61517.07 2017 3153 6512.5 9783.75 17119.0 35516.50 85487.6 483607 36243.96 61840.36 2018 3189 6561.9 9943.50 17334.5 36211.00 86905.8 488698 36756.59 62659.34 2019 3201 6615.6 9812.75 17243.0 36470.50 87146.9 492967 36819.25 63114.77 3.4 Defining New Variables How about if we dont want to calculate summary statistics within a group, but just want to calculate a new variable from each observation? Consider if we had unemployed and labor force levels but did not have unemployment rate, how could we go about calculating it with mutate? indiana&lt;- # 1 indiana %&gt;% # 2 mutate(unemp_rate_calc=round((Unemployed/Labor_force)*100, digits=1)) # 3 Note since we are defining our new data frame as indiana, the one we are manipulating, we are in this case overwriting the indiana data.frame. Since the unemployment rate in our file was listed as percent and rounded to the nearest tenth, I did the same for our calculated. (digits=1 means one decimal place) Now lets compare the first rows of the given unemployment rate with the one we just calculated. ## # A tibble: 6 x 2 ## Unemp_rate unemp_rate_calc ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6.5 6.5 ## 2 5.1 5.1 ## 3 4.8 4.8 ## 4 3.4 3.4 ## 5 9.2 9.2 ## 6 2.4 2.4 3.5 Dropping Variables How can we go about dropping or keeping certain variables? Say we wanted to drop the unemp_rate_calc, Labor_force, and Employed? temp&lt;- # 1 indiana %&gt;% # 2 select(-c(unemp_rate_calc, Labor_force, Employed)) # 3 head(temp) # 4 ## # A tibble: 6 x 7 ## LAUS_code State_fips County_fips County_name Year Unemployed Unemp_rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CN180010000~ 18 001 Adams County,~ 1990 998 6.5 ## 2 CN180030000~ 18 003 Allen County,~ 1990 8317 5.1 ## 3 CN180050000~ 18 005 Bartholomew C~ 1990 1632 4.8 ## 4 CN180070000~ 18 007 Benton County~ 1990 156 3.4 ## 5 CN180090000~ 18 009 Blackford Cou~ 1990 643 9.2 ## 6 CN180110000~ 18 011 Boone County,~ 1990 483 2.4 Now what if we wanted to keep just State_fips, County_fips, Year, and Unemp_rate? temp&lt;- # 1 indiana %&gt;% # 2 select(c(State_fips, County_fips, Year, Unemp_rate)) # 3 head(temp) # 4 ## # A tibble: 6 x 4 ## State_fips County_fips Year Unemp_rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 18 001 1990 6.5 ## 2 18 003 1990 5.1 ## 3 18 005 1990 4.8 ## 4 18 007 1990 3.4 ## 5 18 009 1990 9.2 ## 6 18 011 1990 2.4 That is if we include the - before our variables, we are telling dplyr through our select argument to drop the variables we list. While if we dont have the -, we are telling dplyr to keep only these variables. 3.6 Merging Datasets What if we have 2 data sets, both county level. How could we combine them? 3.6.1 Poverty Estimates We will be merging poverty estimates for the US counties in 2019. A table prepared by the USDA can be found on their website here. Lets download this and put it into our original_data folder. Now lets open it and see what the data looks like. One of the first things we can notice is the first 2 lines, United States and Alabama. That is there is county and state level observations in addition to the county level ones. We need to keep this in mind before we merge with our unemployment data. Try to figure out how to read this data into R by yourself. If you need to review, go back to Downloading/Reading Data. Remember using R Studio to assist with this is probably the simplest way until we get very comfortable with the commands. Here is the command I use for it: PovertyEstimates &lt;- # 1 read_excel(paste0(folder_raw_data, &quot;/PovertyEstimates.xls&quot;), # 2 skip = 4) # 3 Make sure we also have our county unemployment data read in. We can observe both of these data.frames now. The way I would clean this data is the following (of course depends on the question/results you are trying to obtain): unemployment data: restrict to 2019 data (as our other county data is just 2019) Drop Year variable (as we only have one year now) Drop Puerto Rico (State FIPS 72) combine state_fips and county_fips into a FIPStxt variable (this is a variable in our poverty data which we will later be using as an ID to merge data on.) poverty data: filter out state/country level observations, leaving just county level. Here is the code I use for this: unem_2019&lt;-county_data %&gt;% # 1 filter(Year==2019, State_fips!=72) %&gt;% #restrict sample to only year 2019, drop PR # 2 select(-Year) %&gt;% #drop the variable Year # 3 mutate(FIPStxt=paste0(State_fips, County_fips)) #Create variable to merge on # 4 # 5 pov_2019 &lt;- PovertyEstimates %&gt;% # 6 filter(substr(FIPStxt, str_length(FIPStxt)-2, str_length(FIPStxt))!=&quot;000&quot;) # 7 Now if we observe both data frames we are left with what we want to merge. We have defined a variable FIPStxt which we would like to join observations on. That is for a given FIPStxt ID in our unemployment data, we would like to find the row with that FIPStxt ID in our poverty data, then add the variables in the poverty data to our unemployment data. Then do this for all iterations of our FIPStxt ID. There are join commands for this in the dplyr package. The clearest join command is full_join. This matches data frames based on common variable names (or defined variable names to match on.) I print the first 3 rows below. clean_full&lt;- # 1 unem_2019 %&gt;% full_join(pov_2019) # 2 ## Joining, by = &quot;FIPStxt&quot; LAUS_code State_fips County_fips County_name Labor_force Employed Unemployed Unemp_rate FIPStxt Stabr Area_name Rural-urban_Continuum_Code_2003 Urban_Influence_Code_2003 Rural-urban_Continuum_Code_2013 Urban_Influence_Code_2013 POVALL_2019 CI90LBALL_2019 CI90UBALL_2019 PCTPOVALL_2019 CI90LBALLP_2019 CI90UBALLP_2019 POV017_2019 CI90LB017_2019 CI90UB017_2019 PCTPOV017_2019 CI90LB017P_2019 CI90UB017P_2019 POV517_2019 CI90LB517_2019 CI90UB517_2019 PCTPOV517_2019 CI90LB517P_2019 CI90UB517P_2019 MEDHHINC_2019 CI90LBINC_2019 CI90UBINC_2019 POV04_2019 CI90LB04_2019 CI90UB04_2019 PCTPOV04_2019 CI90LB04P_2019 CI90UB04P_2019 CN0100100000000 01 001 Autauga County, AL 26172 25458 714 2.7 01001 AL Autauga County 2 2 2 2 6723 5517 7929 12.1 9.9 14.3 2040 1472 2608 15.9 11.5 20.3 1376 902 1850 14.4 9.4 19.4 58233 52517 63949 NA NA NA NA NA NA CN0100300000000 01 003 Baldwin County, AL 97328 94675 2653 2.7 01003 AL Baldwin County 4 5 3 2 22360 18541 26179 10.1 8.4 11.8 6323 4521 8125 13.5 9.6 17.4 4641 3295 5987 13.3 9.4 17.2 59871 54593 65149 NA NA NA NA NA NA CN0100500000000 01 005 Barbour County, AL 8537 8213 324 3.8 01005 AL Barbour County 6 6 6 6 5909 4787 7031 27.1 22.0 32.2 2050 1560 2540 41.0 31.2 50.8 1468 1114 1822 39.5 30.0 49.0 35972 31822 40122 NA NA NA NA NA NA 3.6.2 left_join While the above full_join command is probably the safest route since it prints all of the variables and rows of both data frames, knowing more about other joins can save us some time. For example, consider above when we first loaded in our poverty data. It had observations at state and country wide levels in addition to the county level data we wanted. We would ideally want a command that took in the county unemployment data, matched the poverty data based on FIPStxt, and then just exclude any extras from the poverty data. We can do this with left (or right) join. unclean_left&lt;- # 1 unem_2019 %&gt;% left_join(PovertyEstimates) # 2 ## Joining, by = &quot;FIPStxt&quot; LAUS_code State_fips County_fips County_name Labor_force Employed Unemployed Unemp_rate FIPStxt Stabr Area_name Rural-urban_Continuum_Code_2003 Urban_Influence_Code_2003 Rural-urban_Continuum_Code_2013 Urban_Influence_Code_2013 POVALL_2019 CI90LBALL_2019 CI90UBALL_2019 PCTPOVALL_2019 CI90LBALLP_2019 CI90UBALLP_2019 POV017_2019 CI90LB017_2019 CI90UB017_2019 PCTPOV017_2019 CI90LB017P_2019 CI90UB017P_2019 POV517_2019 CI90LB517_2019 CI90UB517_2019 PCTPOV517_2019 CI90LB517P_2019 CI90UB517P_2019 MEDHHINC_2019 CI90LBINC_2019 CI90UBINC_2019 POV04_2019 CI90LB04_2019 CI90UB04_2019 PCTPOV04_2019 CI90LB04P_2019 CI90UB04P_2019 CN0100100000000 01 001 Autauga County, AL 26172 25458 714 2.7 01001 AL Autauga County 2 2 2 2 6723 5517 7929 12.1 9.9 14.3 2040 1472 2608 15.9 11.5 20.3 1376 902 1850 14.4 9.4 19.4 58233 52517 63949 NA NA NA NA NA NA CN0100300000000 01 003 Baldwin County, AL 97328 94675 2653 2.7 01003 AL Baldwin County 4 5 3 2 22360 18541 26179 10.1 8.4 11.8 6323 4521 8125 13.5 9.6 17.4 4641 3295 5987 13.3 9.4 17.2 59871 54593 65149 NA NA NA NA NA NA CN0100500000000 01 005 Barbour County, AL 8537 8213 324 3.8 01005 AL Barbour County 6 6 6 6 5909 4787 7031 27.1 22.0 32.2 2050 1560 2540 41.0 31.2 50.8 1468 1114 1822 39.5 30.0 49.0 35972 31822 40122 NA NA NA NA NA NA In this case we are starting with our unemployment data, and then telling R to match all possible values by our common variable (FIPStxt here), then we dont care about any poverty estimate observations that cant be matched. However what about unemployment observations that cant be? We simply fill these values with NAs. For example, if we DONT exclude Puerto Rico, it would be in our unemployment data, but not in our poverty data. Lets see what happens: unem_2019_pr&lt;-county_data %&gt;% # 1 filter(Year==2019) %&gt;% # 2 select(-Year) %&gt;% # 3 mutate(FIPStxt=paste0(State_fips, County_fips)) # 4 pr&lt;- # 5 unem_2019_pr %&gt;% # 6 left_join(PovertyEstimates) %&gt;% # 7 filter(State_fips==72) # 8 ## Joining, by = &quot;FIPStxt&quot; LAUS_code State_fips County_fips County_name Labor_force Employed Unemployed Unemp_rate FIPStxt Stabr Area_name Rural-urban_Continuum_Code_2003 Urban_Influence_Code_2003 Rural-urban_Continuum_Code_2013 Urban_Influence_Code_2013 POVALL_2019 CI90LBALL_2019 CI90UBALL_2019 PCTPOVALL_2019 CI90LBALLP_2019 CI90UBALLP_2019 POV017_2019 CI90LB017_2019 CI90UB017_2019 PCTPOV017_2019 CI90LB017P_2019 CI90UB017P_2019 POV517_2019 CI90LB517_2019 CI90UB517_2019 PCTPOV517_2019 CI90LB517P_2019 CI90UB517P_2019 MEDHHINC_2019 CI90LBINC_2019 CI90UBINC_2019 POV04_2019 CI90LB04_2019 CI90UB04_2019 PCTPOV04_2019 CI90LB04P_2019 CI90UB04P_2019 CN7200100000000 72 001 Adjuntas Municipio, PR 4115 3485 630 15.3 72001 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA CN7200300000000 72 003 Aguada Municipio, PR 11743 10553 1190 10.1 72003 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA CN7200500000000 72 005 Aguadilla Municipio, PR 14350 12881 1469 10.2 72005 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 3.6.3 Additional Resources In addition to full and left join shown here, there is also inner join and right join. Details of these can be found here. "],["exporting.html", "Chapter 4 Exporting Results 4.1 Saving Plots 4.2 Tables to LaTex", " Chapter 4 Exporting Results The goal of this section is to learn how to take the figures/tables we created in the tidyverse section and export them directly into a form we can read into LaTex. Specifically: Xtable ggsave 4.1 Saving Plots See ggplot section in the tidyverse section previously to learn how to generate a plot. Once we have generated a plot we like, lets save it. To do this we will use the ggsave command. ggsave(file=paste0(folder_figures, &quot;/indiana_unemp.png&quot;)) # 1 This will take the last graphic in memory and save it in the location we specify. We can also tell ggplot to make the plot specific dimensions which I highly recommend doing. If you do not specify dimensions, they can be dependent on the size of your viewer in RStudio. ggsave(file=paste0(folder_figures, &quot;/indiana_unemp.png&quot;), # 1 height = 5, width = 7) # 2 4.2 Tables to LaTex Lets learn how to export our summary table we created in the dplyr section. Just as a review here is how we created it. indiana_laborforce&lt;- # 1 # 1 unemp_data %&gt;% # 2 # 2 filter(State_fips==&quot;18&quot;) %&gt;% # 3 # 3 group_by(Year) %&gt;% # 4 # 4 summarise(Min=min(Labor_force), # 5 # 5 p10th=quantile(Labor_force, c(0.1)), # 6 # 6 p25th=quantile(Labor_force, c(0.25)), # 7 # 7 p50th=quantile(Labor_force, c(0.5)), # 8 # 8 p75th=quantile(Labor_force, c(0.75)), # 9 # 9 p90th=quantile(Labor_force, c(0.9)), # 10 # 10 Max=max(Labor_force), # 11 # 11 Average=mean(Labor_force), # 12 # 12 StDev=sd(Labor_force)) # 13 # 13 Now lets load in the package xtable. This package allows us to print data.frames into a tex table environment. Lets first inspect the output for just running the base function with minimal options. xtable(indiana_laborforce) # 1 # 1 ## % latex table generated in R 4.1.0 by xtable 1.8-4 package ## % Mon Jul 19 20:08:02 2021 ## \\begin{table}[ht] ## \\centering ## \\begin{tabular}{rlrrrrrrrrr} ## \\hline ## &amp; Year &amp; Min &amp; p10th &amp; p25th &amp; p50th &amp; p75th &amp; p90th &amp; Max &amp; Average &amp; StDev \\\\ ## \\hline ## 1 &amp; 1990 &amp; 2629.00 &amp; 6639.30 &amp; 9617.50 &amp; 14858.50 &amp; 30089.50 &amp; 60784.50 &amp; 424053.00 &amp; 30593.65 &amp; 52989.19 \\\\ ## 2 &amp; 1991 &amp; 2541.00 &amp; 6380.50 &amp; 9480.00 &amp; 14713.00 &amp; 30131.00 &amp; 62309.90 &amp; 421789.00 &amp; 30357.00 &amp; 52670.08 \\\\ ## 3 &amp; 1992 &amp; 2550.00 &amp; 6284.20 &amp; 9615.50 &amp; 15169.50 &amp; 31117.00 &amp; 63623.20 &amp; 429239.00 &amp; 31058.60 &amp; 53551.02 \\\\ ## 4 &amp; 1993 &amp; 2607.00 &amp; 6200.60 &amp; 10037.75 &amp; 15652.50 &amp; 32034.75 &amp; 64549.90 &amp; 438377.00 &amp; 32046.84 &amp; 54683.97 \\\\ ## 5 &amp; 1994 &amp; 2681.00 &amp; 6453.10 &amp; 10480.50 &amp; 16472.00 &amp; 33808.25 &amp; 66506.30 &amp; 453457.00 &amp; 33308.01 &amp; 56427.35 \\\\ ## 6 &amp; 1995 &amp; 2714.00 &amp; 6543.20 &amp; 10994.75 &amp; 16648.50 &amp; 34588.00 &amp; 67364.50 &amp; 459603.00 &amp; 34078.00 &amp; 57219.27 \\\\ ## 7 &amp; 1996 &amp; 2724.00 &amp; 6522.60 &amp; 10827.50 &amp; 16551.00 &amp; 34597.25 &amp; 66487.50 &amp; 453728.00 &amp; 33800.80 &amp; 56670.37 \\\\ ## 8 &amp; 1997 &amp; 2698.00 &amp; 6585.30 &amp; 10966.00 &amp; 16671.50 &amp; 34786.50 &amp; 66195.30 &amp; 453928.00 &amp; 33919.72 &amp; 56870.28 \\\\ ## 9 &amp; 1998 &amp; 2701.00 &amp; 6629.70 &amp; 11008.75 &amp; 16660.00 &amp; 34150.50 &amp; 65887.60 &amp; 452412.00 &amp; 33947.47 &amp; 56827.82 \\\\ ## 10 &amp; 1999 &amp; 2691.00 &amp; 6496.70 &amp; 10986.75 &amp; 16958.50 &amp; 33968.50 &amp; 65196.00 &amp; 448732.00 &amp; 33965.91 &amp; 56434.89 \\\\ ## 11 &amp; 2000 &amp; 2985.00 &amp; 7053.50 &amp; 10816.50 &amp; 16984.50 &amp; 34996.50 &amp; 63718.60 &amp; 455135.00 &amp; 33982.50 &amp; 56818.02 \\\\ ## 12 &amp; 2001 &amp; 2982.00 &amp; 7085.70 &amp; 10709.00 &amp; 17127.50 &amp; 34674.75 &amp; 64832.00 &amp; 459505.00 &amp; 34140.32 &amp; 57318.59 \\\\ ## 13 &amp; 2002 &amp; 3051.00 &amp; 7040.70 &amp; 10802.75 &amp; 17244.50 &amp; 34974.75 &amp; 66572.30 &amp; 462330.00 &amp; 34469.34 &amp; 57807.67 \\\\ ## 14 &amp; 2003 &amp; 3092.00 &amp; 6941.00 &amp; 10709.50 &amp; 17199.00 &amp; 34514.75 &amp; 67751.50 &amp; 464331.00 &amp; 34597.76 &amp; 58056.22 \\\\ ## 15 &amp; 2004 &amp; 3109.00 &amp; 6815.20 &amp; 10547.00 &amp; 17083.50 &amp; 33431.50 &amp; 68476.20 &amp; 459140.00 &amp; 34432.72 &amp; 57583.09 \\\\ ## 16 &amp; 2005 &amp; 3152.00 &amp; 7075.80 &amp; 10702.75 &amp; 17357.50 &amp; 33606.00 &amp; 69985.60 &amp; 459492.00 &amp; 34841.75 &amp; 57925.42 \\\\ ## 17 &amp; 2006 &amp; 3107.00 &amp; 6883.10 &amp; 10620.50 &amp; 17326.50 &amp; 34554.50 &amp; 72792.00 &amp; 461468.00 &amp; 35164.35 &amp; 58484.53 \\\\ ## 18 &amp; 2007 &amp; 3026.00 &amp; 6816.70 &amp; 10447.50 &amp; 16881.00 &amp; 34304.50 &amp; 73128.10 &amp; 460046.00 &amp; 34866.21 &amp; 58342.88 \\\\ ## 19 &amp; 2008 &amp; 3044.00 &amp; 6816.20 &amp; 10488.00 &amp; 16673.50 &amp; 34065.50 &amp; 75072.00 &amp; 463200.00 &amp; 35131.49 &amp; 58819.13 \\\\ ## 20 &amp; 2009 &amp; 3088.00 &amp; 6758.40 &amp; 10434.25 &amp; 16452.00 &amp; 33700.25 &amp; 74434.10 &amp; 457524.00 &amp; 34717.45 &amp; 58083.80 \\\\ ## 21 &amp; 2010 &amp; 3263.00 &amp; 6440.50 &amp; 10308.00 &amp; 16622.00 &amp; 33921.75 &amp; 74406.00 &amp; 452836.00 &amp; 34512.97 &amp; 57806.16 \\\\ ## 22 &amp; 2011 &amp; 3210.00 &amp; 6459.90 &amp; 10184.25 &amp; 16630.50 &amp; 33804.00 &amp; 75620.50 &amp; 456011.00 &amp; 34586.91 &amp; 58199.53 \\\\ ## 23 &amp; 2012 &amp; 3181.00 &amp; 6336.20 &amp; 10058.50 &amp; 16453.00 &amp; 32924.00 &amp; 76458.00 &amp; 458377.00 &amp; 34454.88 &amp; 58388.80 \\\\ ## 24 &amp; 2013 &amp; 3165.00 &amp; 6325.90 &amp; 9900.50 &amp; 16289.50 &amp; 33053.00 &amp; 78149.50 &amp; 463346.00 &amp; 34656.59 &amp; 58983.65 \\\\ ## 25 &amp; 2014 &amp; 3216.00 &amp; 6401.20 &amp; 9851.50 &amp; 16524.00 &amp; 33215.50 &amp; 80113.30 &amp; 467169.00 &amp; 35052.00 &amp; 59579.47 \\\\ ## 26 &amp; 2015 &amp; 3178.00 &amp; 6341.80 &amp; 9924.75 &amp; 16908.00 &amp; 33960.00 &amp; 82327.70 &amp; 472330.00 &amp; 35508.30 &amp; 60275.44 \\\\ ## 27 &amp; 2016 &amp; 3226.00 &amp; 6444.80 &amp; 9930.50 &amp; 17093.50 &amp; 35149.00 &amp; 85223.00 &amp; 481434.00 &amp; 36178.11 &amp; 61517.07 \\\\ ## 28 &amp; 2017 &amp; 3153.00 &amp; 6512.50 &amp; 9783.75 &amp; 17119.00 &amp; 35516.50 &amp; 85487.60 &amp; 483607.00 &amp; 36243.96 &amp; 61840.36 \\\\ ## 29 &amp; 2018 &amp; 3189.00 &amp; 6561.90 &amp; 9943.50 &amp; 17334.50 &amp; 36211.00 &amp; 86905.80 &amp; 488698.00 &amp; 36756.59 &amp; 62659.34 \\\\ ## 30 &amp; 2019 &amp; 3201.00 &amp; 6615.60 &amp; 9812.75 &amp; 17243.00 &amp; 36470.50 &amp; 87146.90 &amp; 492967.00 &amp; 36819.25 &amp; 63114.77 \\\\ ## \\hline ## \\end{tabular} ## \\end{table} We could then copy and paste this output into latex. However, we can tell xtable and R to save a tex file with this in it. Notice the pathing used. This is a latex table so we are keeping ourselves organized by 1) giving the tex file a useful name and 2) saving it within our tables folder. This may seem extra tedious now but come generating hundreds of plots and tables, organazation can be key to keeping everything straight. print.xtable( # 1 # 1 xtable(indiana_laborforce), # 2 # 2 file=paste0(workingdir, &quot;tables/indiana_laborforce.tex&quot;)) # 3 # 3 {{% callout note %}} Try building this table in a pdf through LaTex {{% /callout %}} Lets now customize this table more. First, lets get rid of row names as they are not meaningful for us here. Also, if we want to later fix this table in a certain place in a tex document we want a table placement H. print.xtable( # 1 # 1 xtable(indiana_laborforce), # 2 # 2 table.placement=&quot;H&quot;, # 3 # 3 include.rownames=FALSE, # 4 # 4 file=paste0(workingdir, &quot;tables/indiana_laborforce.tex&quot;), # 5 # 5 ) # 6 # 6 Lets add a bit more customization to this table. print.xtable( # 1 # 1 xtable( # 2 # 2 indiana_laborforce, # 3 # 3 digits=0, # 4 # 4 label= &quot;tab:indiana_laborforce&quot;, # 5 # 5 align = c(&quot;|c|&quot;,&quot;|c|&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c|&quot;)), # 6 # 6 table.placement=&quot;H&quot;, # 7 # 7 include.rownames=FALSE, # 8 # 8 file=paste0(folder_tables, &quot;/indiana_laborforce.tex&quot;)) # 9 # 9 Line #4: Round all digits to the nearest integer. Line #5: add a label to the table which you can then reference to in your main tex document. Line #6: if we want to have a custom alignment for our table. (Note: this command requires 1 more column of arguments than there is columns in your table. This is due to it printing the row names. However if you later define include.rownames as FALSE as I do here. It will get rid of the first column of row names.) There are many other packages that can assist in creating tex tables. We will later use stargazer for reporting regression results. I like using xtable due to its ease to customize. Please leave me a note below if you have any favorite packages for creating tex tables. "],["GIS.html", "Chapter 5 GIS Example 5.1 MIT Election Data 5.2 Pollution Data 5.3 BLS County Statistics 5.4 Combining Data", " Chapter 5 GIS Example This chapter looks at some GIS work I did for a research project in progress. To investigate the question of whether a politicians margin of victory is related to pollution outcomes I assemble a dataset combining data from EPA pollution monitors, election results, congressional district boundaries, county level variables, weather data and matching them together by geography 5.1 MIT Election Data Data on U.S. House elections from 1976-2016 is obtained from MITs Election Data and Science Lab. The data contains the name of candidates running in a US House election, the state and district they are running in, their party affiliation, if they were a write in candidate, how many votes they received, how many votes were cast in the election overall, and if the election was a special or regular election. From this data allows me to define incumbency variables grouping incumbents into the number of terms they served. Since this data spans back to only 1976, any incumbency measure is based on 1976 as a starting point. Thus if I wanted to group incumbents serving 3 terms, I would check which politicians served in three previous terms. For example, in 1980 I would see if a given victorious politician had also won elections in 1976 and 1978. If they had I would categorize them as a 3 term Representative. Should I want to group 5 term incumbents, I would not be able to to this in 1980 since I would need data going back to 1972. In some cases in my analysis I will have to drop some pollution observations at the beginning of my sample for this reason. I also calculate the margin of victory from this data defined as the difference in votes of the top two candidates divided by the sum of the top two. For example consider a race of three candidates, a Republican victor who has 45% of the vote, a Democrat who wins 40% of the vote, and an Independent who wins the remaining 15% of the vote. Calculating the margin of victory would give us: \\[ \\frac{0.45 - 0.40}{0.45 + 0.40} \\approx 0.0588 \\] Results of elections are matched to pollution monitors through mapping the monitors into congressional districts and then attributing monitors to districts which politicians represent. Historic district boundary shapefiles are obtained from the website of Lewis et al (2013). These maps allow me to overlay the pollution monitors latitude and longitude coordinates, obtaining which district each monitor is located. Example of boundaries for the 110th congress (contiguous US) below. 5.2 Pollution Data Ozone pollution data is obtained from the EPAs website on Air Quality at the daily frequency. Monitors collect observations through the course of the day on the parts per million of ground level ozone, they are then averaged by consecutive 8-hour readings. I utilize the maximum 8-hour reading for each day, restricting my sample to reliable monitors within the month defined as having more than 25 day readings within July. I then average the top 5 days within the month of July. Below I illustrate percentiles. 5.3 BLS County Statistics Data on yearly population, personal income, and employment data at the county level were obtained from the Bureau of Economic Analysis. Utilizing this data in combination with county shapefiles from the Census Bureau allowed me to calculate a population density for each county. 5.4 Combining Data Pollution monitors are a point in space while congressional districts and counties are an area. Hence we will map these points into both of these boundaries. In the below image I illustrate this: the colored areas represent counties, the dark boundaries represent congressional districts, and the points are pollution monitor locations. "]]
